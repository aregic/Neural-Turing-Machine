{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing NTM performance on copy task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from ntm import NTM\n",
    "from recurrent_controller import RecurrentController\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(batch_size, length, size):\n",
    "    input_data = np.zeros((batch_size, 2 * length + 2, size), dtype=np.float32)\n",
    "    target_output = np.zeros((batch_size, 2 * length + 2, size), dtype=np.float32)\n",
    "\n",
    "    sequence = np.random.binomial(1, 0.5, (batch_size, length, size - 2))\n",
    "    input_data[:, 0, 0] = 1\n",
    "    input_data[:, 1:length+1, 1:size-1] = sequence\n",
    "    input_data[:, length+1, -1] = 1  # the end symbol\n",
    "    target_output[:, length + 2:, 1:size-1] = sequence\n",
    "\n",
    "    return input_data, target_output\n",
    "\n",
    "def llprint(message):\n",
    "    sys.stdout.write(message)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(predictions, targets):\n",
    "    return tf.reduce_mean(-1 * targets * tf.log(predictions) - (1 - targets) * tf.log(1 - predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_op(input_series, ntm_output, memory_view):\n",
    "    gs = gridspec.GridSpec(20, 1, hspace=0)\n",
    "\n",
    "    ww_strip = np.squeeze(memory_view['write_weightings'])\n",
    "    rw_strip = np.squeeze(memory_view['read_weightings'])\n",
    "    colored_write = np.zeros((ww_strip.shape[0], ww_strip.shape[1], 3))\n",
    "    colored_read = np.zeros((rw_strip.shape[0], rw_strip.shape[1], 3))\n",
    "    for i in range(ww_strip.shape[0]):\n",
    "        for j in range(ww_strip.shape[1]):\n",
    "            colored_read[i, j] = [rw_strip[i,j], 0., 0.]\n",
    "            colored_write[i, j] = [0., ww_strip[i,j], 0.]\n",
    "\n",
    "\n",
    "    iax = plt.subplot(gs[0:5, 0])\n",
    "    oax = plt.subplot(gs[5:10, 0])\n",
    "    memax = plt.subplot(gs[10:, 0])\n",
    "\n",
    "    iax.grid(True, color='gray')\n",
    "    oax.grid(True, color='gray')\n",
    "    memax.grid(True, color='gray', axis='x')\n",
    "        \n",
    "    iax.imshow(np.squeeze(input_series.T), cmap=plt.cm.gray, interpolation='nearest')\n",
    "    iax.set_ylabel(\"Inputs\")\n",
    "    iax.set_yticks([])\n",
    "\n",
    "    oax.imshow(np.squeeze(ntm_output.T), cmap=plt.cm.gray, interpolation='nearest')\n",
    "    oax.set_ylabel(\"Outputs\")\n",
    "    oax.set_yticks([])\n",
    "\n",
    "    memax.imshow(np.transpose(colored_write + colored_read, [1, 0, 2]), interpolation='nearest')\n",
    "    memax.set_ylabel(\"Memory Location\")\n",
    "    write_legend = mpatches.Rectangle((1,1), 1, 1, color='green', label='Write Head')\n",
    "    read_legend = mpatches.Rectangle((1,1), 1, 1, color='red', label='Read Head')\n",
    "    memax.legend(bbox_to_anchor=(0.21, -0.1), handles=[write_legend, read_legend])\n",
    "    \n",
    "    return colored_write, colored_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(s1, s2):\n",
    "    \"\"\"Return the Hamming distance between equal-length sequences\"\"\"\n",
    "    if len(s1) != len(s2):\n",
    "        raise ValueError(\"Undefined for sequences of unequal length\")\n",
    "    return sum(el1 != el2 for el1, el2 in zip(s1, s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and running NTM model\n",
    "\n",
    "Trained on sequences of length up to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_seq_length = 30    # Sequence length for testing\n",
    "testing_runs = 10          # Number of testing runs\n",
    "\n",
    "dist = []\n",
    "losses = []\n",
    "matches = []\n",
    "inputs = []\n",
    "outs = []\n",
    "views = []\n",
    "\n",
    "ckpts_dir = os.path.join(os.path.dirname(\"__file__\"), 'checkpoints')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "    turing_machine = NTM(\n",
    "        RecurrentController,\n",
    "        input_size = 10,\n",
    "        output_size = 10,\n",
    "        memory_locations = 128,\n",
    "        memory_word_size = 20,\n",
    "        memory_read_heads = 1,\n",
    "        shift_range = 1,\n",
    "        batch_size = 1\n",
    "    )\n",
    "    \n",
    "    outputs, memory_views = turing_machine.get_outputs()\n",
    "    squashed_output = tf.clip_by_value(tf.sigmoid(outputs), 1e-6, 1. - 1e-6)\n",
    "    loss = binary_cross_entropy(squashed_output, turing_machine.target_output)\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Restoring the provided model\n",
    "    turing_machine.restore(session, ckpts_dir, 'step-100000') \n",
    "    \n",
    "    for i in range(testing_runs):\n",
    "        input_data, target_output = generate_data(1, testing_seq_length, 10)\n",
    "\n",
    "        loss_value,out,mem = session.run([\n",
    "            loss,\n",
    "            squashed_output,\n",
    "            memory_views\n",
    "        ], feed_dict={\n",
    "            turing_machine.input_data: input_data,\n",
    "            turing_machine.target_output: target_output,\n",
    "            turing_machine.sequence_length: testing_seq_length*2 + 2\n",
    "        })\n",
    "        \n",
    "        dist.append(hamming_distance(np.reshape(np.round(out),(1,-1)).tolist()[0],np.reshape(target_output,(1,-1)).tolist()[0]))\n",
    "        losses.append(loss_value)\n",
    "        inputs.append(input_data)\n",
    "        outs.append(out)\n",
    "        views.append(mem)\n",
    "        matches.append(np.allclose(target_output, np.around(out)))\n",
    "        \n",
    "print(\"Avg. Accuracy: %.4f\" % (np.mean(matches)))\n",
    "print(\"Avg. Loss: %.4f\" % (np.mean(losses)))\n",
    "print(\"Avg. Dist: %.4f\" % (np.mean(dist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the best output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_indx = np.argmin(dist)\n",
    "print('Hamming distance: %d' % dist[best_indx])\n",
    "best_input, best_output, best_memview = inputs[best_indx], outs[best_indx], views[best_indx]\n",
    "\n",
    "a = visualize_op(best_input, best_output, best_memview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
